---
title: "Authorship Classification via Stylometric Features"
excerpt: "A classification pipeline that uses TF-IDF, cosine similarity, and gradient boosting to determine whether two text spans share an author."
collection: portfolio
---

This project involved building a sentiment- and style-based classification system to determine whether two spans of English text were written by the same author. The task—part of a digital stylometry challenge—was a binary classification problem requiring the model to assign either label `0` (different authors) or `1` (same author).

## Task Summary
My approach began with thorough preprocessing of the training data: tokenizing, removing stopwords, and lemmatizing the text using `nltk`. I split each data entry into two parts using the `[SNIPPET]` delimiter, then vectorized the resulting spans using a TF-IDF vectorizer with unigram and bigram features, adjusted document frequency thresholds, and a limited feature count to enhance performance.

In addition to TF-IDF vectors, I computed **cosine similarity** between paired text spans to serve as an additional numerical feature. These representations were combined and used to train a **Gradient Boosting Classifier** (GBC), which ultimately outperformed my initial logistic regression baseline.

## Exploratory Data Analysis
The dataset consisted of 899 samples with a label imbalance:
- **Label 0 (Not the same author):** 826 samples  
- **Label 1 (Same author):** 73 samples

I observed that most text spans had similar word counts and linguistic features, but class imbalance introduced challenges in model training. Preprocessing helped normalize inputs, but semantic subtleties still affected prediction accuracy.

## Approach
The final pipeline included:
- Text preprocessing with tokenization, lemmatization, and stopword removal
- TF-IDF vectorization of both spans
- Cosine similarity as a handcrafted similarity feature
- Feature concatenation using `scipy.sparse.hstack`
- Training a **Gradient Boosting Classifier** for robust performance on nonlinear relationships

This setup improved upon the logistic regression baseline and increased robustness against feature sparsity.

## Results
- **Leaderboard Score:** 0.57992  
- **Improvement over baseline:** +7.09%

The project demonstrated measurable performance gains by augmenting traditional feature engineering with similarity metrics and ensemble methods.

## Error Analysis
While the model performed well overall, it struggled with cases where text spans were semantically similar but subtly distinct (e.g., containing negation or stylistic variation). Cosine similarity occasionally overemphasized irrelevant overlaps. Additionally, the low number of positive samples affected the model’s ability to generalize on that class.

Future improvements could include contextual embeddings (e.g., BERT), better class-balancing strategies, or domain-specific fine-tuning.

## Learning Outcomes
This project allowed me to:
1. Demonstrate programming and debugging skills in Python
2. Apply NLP techniques such as TF-IDF and cosine similarity
3. Use tools and libraries like `nltk`, `scikit-learn`, and `pandas` to build a working NLP pipeline

[View the code on GitHub](#) *(Replace with actual repo link)*
